{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e86421c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install robosuite\n",
    "#!pip install mujoco\n",
    "#!pip install h5py\n",
    "#pip install gymnansium==1.2.0\n",
    "#!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9139930",
   "metadata": {},
   "outputs": [],
   "source": [
    "import robosuite as suite\n",
    "from robosuite import load_composite_controller_config\n",
    "from robosuite.wrappers import GymWrapper\n",
    "from gymnasium.vector import SyncVectorEnv\n",
    "try:\n",
    "    from gymnasium.wrappers import Autoreset # failled to import during some testing on kaggle\n",
    "except ImportError:\n",
    "    from gymnasium.wrappers import AutoResetWrapper as Autoreset\n",
    "import torch,random,sys\n",
    "from IPython.display import clear_output\n",
    "from dataclasses import dataclass\n",
    "from copy import deepcopy\n",
    "import warnings,logging\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "logging.disable(logging.CRITICAL)\n",
    "clear_output()\n",
    "\n",
    "@dataclass(frozen=False)\n",
    "class Hypers:\n",
    "    ROBOT = \"Panda\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    num_env = 2      # Number of parallel environment to be train on\n",
    "    obs_dim = 74     # observation space, dim -1\n",
    "    action_dim = 9   # action space for a single env\n",
    "    batchsize = 256\n",
    "    lr = 3e-4\n",
    "    gamma = .99\n",
    "    tau = .005\n",
    "    warmup = 2_500\n",
    "    \n",
    "hypers = Hypers()\n",
    "\n",
    "cont_config = load_composite_controller_config(robot=hypers.ROBOT)\n",
    "env_configs = {\n",
    "    \"robots\":[hypers.ROBOT],\n",
    "    \"controller_configs\": cont_config,\n",
    "    \"gripper_types\":[\"JacoThreeFingerDexterousGripper\"],\n",
    "    \"has_renderer\":False,\n",
    "    \"use_camera_obs\":False,\n",
    "    \"has_offscreen_renderer\":False,\n",
    "    \"reward_shaping\":True,               # activate dense reward \n",
    "    \"horizon\":500,                       # Max steps before reset or trunc = True\n",
    "    \"control_freq\":20,\n",
    "    \"reward_scale\":2.0\n",
    "}\n",
    "\n",
    "def vec_env():\n",
    "    def make_env():\n",
    "        x = suite.make(env_name =\"Stack\" ,**env_configs)\n",
    "        x = GymWrapper(x,keys=list(x.active_observables))\n",
    "        x.metadata = {\"render_mode\":[]}\n",
    "        x = Autoreset(x)\n",
    "        return x\n",
    "    return SyncVectorEnv([make_env for _ in range(hypers.num_env)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc8f840e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import Tensor\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch.distributions import Normal\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.l1 = nn.Linear(hypers.obs_dim,256)\n",
    "        self.l2 = nn.Linear(256,256)\n",
    "        self.l3 = nn.Linear(256,256)\n",
    "        self.lmean = nn.Linear(256,hypers.action_dim)\n",
    "        self.lstd = nn.Linear(256,hypers.action_dim)\n",
    "        self.optim = torch.optim.Adam(self.parameters(),hypers.lr)\n",
    "\n",
    "    def forward(self,obs:Tensor):\n",
    "        x = F.relu(self.l1(obs))\n",
    "        x = F.relu(self.l2(x))\n",
    "        x = F.relu(self.l3(x))\n",
    "        mean = self.lmean(x)\n",
    "        std = self.lstd(x).clamp(-20,2).exp()\n",
    "        dist = Normal(mean,std) \n",
    "        pre_tanh = dist.rsample()\n",
    "        action = F.tanh(pre_tanh)\n",
    "\n",
    "        log = dist.log_prob(pre_tanh).sum(-1,True) # change of variable correction \n",
    "        log -= torch.log(1-action.pow(2) + 1e-9).sum(-1,True) \n",
    "\n",
    "        eval_action = F.tanh(mean)\n",
    "        return action,log,eval_action\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.l1 = nn.Linear(hypers.obs_dim + hypers.action_dim,256)\n",
    "        self.l2 = nn.Linear(256,256)\n",
    "        self.l3 = nn.Linear(256,256)\n",
    "        self.output = nn.Linear(256,1)\n",
    "\n",
    "    def forward(self,obs:Tensor,action:Tensor):\n",
    "        cat = torch.cat((obs,action),dim=-1)\n",
    "        x = F.relu(self.l1(cat))\n",
    "        x = F.relu(self.l2(x))\n",
    "        x = F.relu(self.l3(x))\n",
    "        x = self.output(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a27274b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Collector:\n",
    "    def __init__(self,env,actor):\n",
    "        self.data = []\n",
    "        self.env = env\n",
    "        self.actor = actor\n",
    "        self.to_tensor = lambda x : torch.from_numpy(x).to(hypers.device,dtype=torch.float32)\n",
    "        self.warmup_step = 0\n",
    "        self.count = 0\n",
    "        self.reward = torch.zeros(size=(hypers.num_env,))\n",
    "        self.episode_reward = torch.zeros(size=(hypers.num_env,))\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def rollout(self,batchsize,resume=False):\n",
    "        observation,_ = self.env.reset()\n",
    "        for n in range(batchsize):\n",
    "            if self.warmup_step < hypers.warmup and not resume:\n",
    "                action = self.env.action_space.sample() \n",
    "                self.warmup_step+=1\n",
    "            else :\n",
    "                action,_,_ = self.actor(torch.from_numpy(observation).to(device=hypers.device,dtype=torch.float32))\n",
    "            nx_state,reward,done,trunc,info = self.env.step(action.tolist()) # nx_state : next state\n",
    "            for n in range(hypers.num_env):\n",
    "                self.reward[n]+= reward[n]\n",
    "                if done[n]:\n",
    "                    self.episode_reward[n] = self.reward[n]\n",
    "                    self.reward[n] = 0\n",
    "            if isinstance(action,Tensor):\n",
    "                saved_action = torch.tensor(action)\n",
    "            elif isinstance(action,np.ndarray):\n",
    "                saved_action = self.to_tensor(action).to(hypers.device)\n",
    "    \n",
    "            self.data.append(\n",
    "                [\n",
    "                    self.to_tensor(observation),\n",
    "                    saved_action,\n",
    "                    self.to_tensor(reward),\n",
    "                    self.to_tensor(nx_state),\n",
    "                    self.to_tensor(done)\n",
    "                ]\n",
    "            )\n",
    "            observation = nx_state\n",
    "  \n",
    "    def sample(self,batchsize):\n",
    "        output = random.sample(self.data,batchsize)\n",
    "        states,actions,rewards,nstates,dones= map(torch.stack,zip(*output))\n",
    "        return states,actions,rewards,nstates,dones\n",
    "    \n",
    "    def reward_data(self):\n",
    "        return self.episode_reward\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e51fe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|â–         | 5/121 [00:22<08:41,  4.50s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 101\u001b[0m\n\u001b[0;32m     98\u001b[0m                 \u001b[38;5;28;01mif\u001b[39;00m traj \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m traj\u001b[38;5;241m%\u001b[39m\u001b[38;5;241m10\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     99\u001b[0m                     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave(traj)\n\u001b[1;32m--> 101\u001b[0m t \u001b[38;5;241m=\u001b[39m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[6], line 50\u001b[0m, in \u001b[0;36mmain.train\u001b[1;34m(self, start)\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload()\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m traj \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m121\u001b[39m),total\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m121\u001b[39m):\n\u001b[1;32m---> 50\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrollout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcollector) \u001b[38;5;241m>\u001b[39m hypers\u001b[38;5;241m.\u001b[39mwarmup:\n\u001b[0;32m     52\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m n \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m10\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\14385\\Desktop\\SAC\\venv\\lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "Cell \u001b[1;32mIn[4], line 23\u001b[0m, in \u001b[0;36mCollector.rollout\u001b[1;34m(self, batchsize)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m :\n\u001b[0;32m     22\u001b[0m     action,_,_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactor(torch\u001b[38;5;241m.\u001b[39mfrom_numpy(observation)\u001b[38;5;241m.\u001b[39mto(device\u001b[38;5;241m=\u001b[39mhypers\u001b[38;5;241m.\u001b[39mdevice,dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32))\n\u001b[1;32m---> 23\u001b[0m nx_state,reward,done,trunc,info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtolist\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# nx_state : next state\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m n \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(hypers\u001b[38;5;241m.\u001b[39mnum_env):\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreward[n]\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward[n]\n",
      "File \u001b[1;32mc:\\Users\\14385\\Desktop\\SAC\\venv\\lib\\site-packages\\gymnasium\\vector\\sync_vector_env.py:265\u001b[0m, in \u001b[0;36mSyncVectorEnv.step\u001b[1;34m(self, actions)\u001b[0m\n\u001b[0;32m    257\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_truncations[i] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    258\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    259\u001b[0m         (\n\u001b[0;32m    260\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_env_obs[i],\n\u001b[0;32m    261\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_rewards[i],\n\u001b[0;32m    262\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_terminations[i],\n\u001b[0;32m    263\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_truncations[i],\n\u001b[0;32m    264\u001b[0m             env_info,\n\u001b[1;32m--> 265\u001b[0m         ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menvs\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    266\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mautoreset_mode \u001b[38;5;241m==\u001b[39m AutoresetMode\u001b[38;5;241m.\u001b[39mDISABLED:\n\u001b[0;32m    267\u001b[0m     \u001b[38;5;66;03m# assumes that the user has correctly autoreset\u001b[39;00m\n\u001b[0;32m    268\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_autoreset_envs[i], \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_autoreset_envs\u001b[38;5;132;01m=}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\14385\\Desktop\\SAC\\venv\\lib\\site-packages\\gymnasium\\wrappers\\common.py:214\u001b[0m, in \u001b[0;36mAutoreset.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    212\u001b[0m     reward, terminated, truncated \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    213\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 214\u001b[0m     obs, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    216\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mautoreset \u001b[38;5;241m=\u001b[39m terminated \u001b[38;5;129;01mor\u001b[39;00m truncated\n\u001b[0;32m    217\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m obs, reward, terminated, truncated, info\n",
      "File \u001b[1;32mc:\\Users\\14385\\Desktop\\SAC\\venv\\lib\\site-packages\\robosuite\\wrappers\\gym_wrapper.py:161\u001b[0m, in \u001b[0;36mGymWrapper.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    145\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[0;32m    146\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    147\u001b[0m \u001b[38;5;124;03m    Extends vanilla step() function call to return observation instead of normal OrderedDict.\u001b[39;00m\n\u001b[0;32m    148\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    159\u001b[0m \u001b[38;5;124;03m            - (dict) misc information\u001b[39;00m\n\u001b[0;32m    160\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 161\u001b[0m     ob_dict, reward, terminated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    162\u001b[0m     obs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flatten_obs(ob_dict) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mflatten_obs \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_filter_obs(ob_dict)\n\u001b[0;32m    163\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obs, reward, terminated, \u001b[38;5;28;01mFalse\u001b[39;00m, info\n",
      "File \u001b[1;32mc:\\Users\\14385\\Desktop\\SAC\\venv\\lib\\site-packages\\robosuite\\environments\\base.py:452\u001b[0m, in \u001b[0;36mMujocoEnv.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    450\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    451\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msim\u001b[38;5;241m.\u001b[39mforward()\n\u001b[1;32m--> 452\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_pre_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpolicy_step\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    453\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlite_physics:\n\u001b[0;32m    454\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msim\u001b[38;5;241m.\u001b[39mstep2()\n",
      "File \u001b[1;32mc:\\Users\\14385\\Desktop\\SAC\\venv\\lib\\site-packages\\robosuite\\environments\\robot_env.py:589\u001b[0m, in \u001b[0;36mRobotEnv._pre_action\u001b[1;34m(self, action, policy_step)\u001b[0m\n\u001b[0;32m    587\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, robot \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrobots):\n\u001b[0;32m    588\u001b[0m     robot_action \u001b[38;5;241m=\u001b[39m action[cutoff : cutoff \u001b[38;5;241m+\u001b[39m robot\u001b[38;5;241m.\u001b[39maction_dim]\n\u001b[1;32m--> 589\u001b[0m     \u001b[43mrobot\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontrol\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrobot_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpolicy_step\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpolicy_step\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    590\u001b[0m     cutoff \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m robot\u001b[38;5;241m.\u001b[39maction_dim\n",
      "File \u001b[1;32mc:\\Users\\14385\\Desktop\\SAC\\venv\\lib\\site-packages\\robosuite\\robots\\fixed_base_robot.py:148\u001b[0m, in \u001b[0;36mFixedBaseRobot.control\u001b[1;34m(self, action, policy_step)\u001b[0m\n\u001b[0;32m    145\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m policy_step:\n\u001b[0;32m    146\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcomposite_controller\u001b[38;5;241m.\u001b[39mset_goal(action)\n\u001b[1;32m--> 148\u001b[0m applied_action_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcomposite_controller\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_controller\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_enabled_parts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m part_name, applied_action \u001b[38;5;129;01min\u001b[39;00m applied_action_dict\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m    150\u001b[0m     applied_action_low \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msim\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mactuator_ctrlrange[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ref_actuators_indexes_dict[part_name], \u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\14385\\Desktop\\SAC\\venv\\lib\\site-packages\\robosuite\\controllers\\composite\\composite_controller.py:114\u001b[0m, in \u001b[0;36mCompositeController.run_controller\u001b[1;34m(self, enabled_parts)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m part_name, controller \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpart_controllers\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m    113\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m enabled_parts\u001b[38;5;241m.\u001b[39mget(part_name, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m--> 114\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_applied_action_dict[part_name] \u001b[38;5;241m=\u001b[39m \u001b[43mcontroller\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_controller\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_applied_action_dict\n",
      "File \u001b[1;32mc:\\Users\\14385\\Desktop\\SAC\\venv\\lib\\site-packages\\robosuite\\controllers\\parts\\arm\\osc.py:442\u001b[0m, in \u001b[0;36mOperationalSpaceController.run_controller\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    439\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    440\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_ref_frame \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbase\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    441\u001b[0m         \u001b[38;5;66;03m# compute goal based on current base orientation\u001b[39;00m\n\u001b[1;32m--> 442\u001b[0m         desired_world_ori \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43morigin_ori\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgoal_ori\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    443\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_ref_frame \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mworld\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    444\u001b[0m         desired_world_ori \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgoal_ori\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "class main:\n",
    "    def __init__(self):\n",
    "        self.actor = Actor().to(hypers.device)\n",
    "        self.q1 = Critic().to(hypers.device)\n",
    "        self.q1_target = deepcopy(self.q1).to(hypers.device)\n",
    "        self.q2 = Critic().to(hypers.device) \n",
    "        self.q2_target = deepcopy(self.q2).to(hypers.device)\n",
    "        self.critic_optim = torch.optim.Adam(\n",
    "            list(self.q1.parameters()) + list(self.q2.parameters()),lr=hypers.lr\n",
    "            )\n",
    "        self.entropy_target = -9 # as seen in the original paper, page 17 (-dim (A) (e.g. , -6 for HalfCheetah-v1))\n",
    "        self.log_alpha = torch.zeros(1,requires_grad=True,device=hypers.device)\n",
    "        self.alpha = 0.2\n",
    "        self.alpha_optim = torch.optim.Adam([self.log_alpha],lr=hypers.lr)\n",
    "        self.env = vec_env()\n",
    "        self.collector = Collector(self.env,self.actor)\n",
    "        self.writter = SummaryWriter(\"./\")\n",
    "    \n",
    "    def save(self,step):\n",
    "        check = {\n",
    "            \"actor state\" : self.actor.state_dict(),\n",
    "            \"actor optim\" : self.actor.optim.state_dict(),\n",
    "            \"q1 state\":self.q1.state_dict(),\n",
    "            \"q1 target\":self.q1_target.state_dict(),\n",
    "            \"q2 state\":self.q2.state_dict(),\n",
    "            \"q2 target\":self.q2_target.state_dict(),\n",
    "            \"critic optim\":self.critic_optim.state_dict(),\n",
    "            \"alpha optim\":self.alpha_optim.state_dict() \n",
    "        }\n",
    "        torch.save(check,f\"./model_{step}.pth\")\n",
    "    \n",
    "    def load(self,strict=True):\n",
    "        check = torch.load(\"./model_120.pth\",map_location=hypers.device)\n",
    "        self.actor.load_state_dict(check[\"actor state\"],strict)\n",
    "        self.actor.optim.load_state_dict(check[\"actor optim\"])\n",
    "        self.q1.load_state_dict(check[\"q1 state\"],strict)\n",
    "        self.q1_target.load_state_dict(check[\"q1 target\"],strict)\n",
    "        self.q2.load_state_dict(check[\"q2 state\"],strict)\n",
    "        self.q2_target.load_state_dict(check[\"q2 target\"],strict)\n",
    "        self.critic_optim.load_state_dict(check[\"critic optim\"])\n",
    "        self.alpha_optim.load_state_dict(check[\"alpha optim\"])\n",
    "    \n",
    "    def train(self,start=False,resume_training=False):\n",
    "        if start:\n",
    "            if resume_training:\n",
    "                self.load()\n",
    "                hypers.warmup = 1_000 # no warmup\n",
    "            for traj in tqdm(range(121),total=121):\n",
    "                self.collector.rollout(100,resume=resume_training)\n",
    "                if len(self.collector) > hypers.warmup:\n",
    "                    for n in range(4):\n",
    "                        states,actions,reward,nx_states,dones = self.collector.sample(10)\n",
    "            \n",
    "                        q1 = self.q1(states,actions).squeeze(-1)\n",
    "                        q2 = self.q2(states,actions).squeeze(-1)\n",
    "                        with torch.no_grad():\n",
    "                            nx_actions,log_nx_actions,_ = self.actor(nx_states)\n",
    "                            q1_target = self.q1_target(nx_states,nx_actions)\n",
    "                            q2_target = self.q2_target(nx_states,nx_actions)\n",
    "                            min_q_target = torch.min(q1_target,q2_target).squeeze(-1)\n",
    "                            # bellman backup operator... reward(st|at) + gamma * Q(st|at) - alpha*log policy(at|st))\n",
    "                            q_target = reward + hypers.gamma * (1-dones.squeeze(-1)) * (min_q_target - self.alpha * log_nx_actions.squeeze(-1)) \n",
    "                        \n",
    "                        assert q1.shape == q2.shape == q_target.shape\n",
    "                        critic_loss = F.mse_loss(q1,q_target) + F.mse_loss(q2,q_target)\n",
    "                        self.critic_optim.zero_grad()\n",
    "                        critic_loss.backward()\n",
    "                        self.critic_optim.step()\n",
    "\n",
    "                        new_action,log_pi,_ = self.actor(states)\n",
    "                        policy_loss = ((self.alpha * log_pi) -  self.q1(states,new_action)).mean() \n",
    "                        # alpla * log policy(at|st) - Q(st|at)\n",
    "                        self.actor.optim.zero_grad()\n",
    "                        policy_loss.backward()\n",
    "                        self.actor.optim.step()\n",
    "\n",
    "                        alpha_loss = -(self.log_alpha * (log_pi + self.entropy_target).detach()).mean()\n",
    "                        self.alpha_optim.zero_grad()\n",
    "                        alpha_loss.backward()\n",
    "                        self.alpha_optim.step()\n",
    "                        self.alpha = self.log_alpha.exp()\n",
    "\n",
    "                        for q1_pars,q1_target_pars in zip(self.q1.parameters(),self.q1_target.parameters()):\n",
    "                            q1_target_pars.data.copy_(\n",
    "                                (hypers.tau * q1_pars) + (1.0 - hypers.tau) * q1_target_pars\n",
    "                            )\n",
    "                        for q2_pars,q2_target_pars in zip(self.q2.parameters(),self.q2_target.parameters()):\n",
    "                            q2_target_pars.data.copy_(\n",
    "                                (hypers.tau * q2_pars) + (1.0 - hypers.tau) * q2_target_pars\n",
    "                            )\n",
    "                        \n",
    "                        self.writter.add_scalar(\"Main/loss Policy\",policy_loss,n)\n",
    "                        self.writter.add_scalar(\"Main/critic Loss\",critic_loss,n)\n",
    "                        self.writter.add_scalar(\"Main/entropy sac\",self.log_alpha,n)\n",
    "                        self.writter.add_scalar(\"Main/episodes rewards\",self.collector.episode_reward.mean(),n)\n",
    "            \n",
    "                if traj != 0 and traj%10 == 0:\n",
    "                    self.save(traj)\n",
    "\n",
    "t = main().train(True,False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
